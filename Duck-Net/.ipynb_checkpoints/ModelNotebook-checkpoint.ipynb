{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4653dc2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 90152,
     "status": "ok",
     "timestamp": 1702553771205,
     "user": {
      "displayName": "Azimi2kht",
      "userId": "01921100736619125997"
     },
     "user_tz": -210
    },
    "id": "0ZbOmGM8D3hr",
    "outputId": "1e802c4d-6a78-40bb-ae84-880973386fe7"
   },
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b7b7a5",
   "metadata": {
    "executionInfo": {
     "elapsed": 7186,
     "status": "ok",
     "timestamp": 1702553778384,
     "user": {
      "displayName": "Azimi2kht",
      "userId": "01921100736619125997"
     },
     "user_tz": -210
    },
    "id": "857a5f70"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 14:02:19.223034: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-16 14:02:19.403384: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-05-16 14:02:19.403406: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-05-16 14:02:19.453798: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-16 14:02:20.501778: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-16 14:02:20.501889: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-16 14:02:20.501901: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary libraries\n",
    "import tensorflow as tf\n",
    "import albumentations as albu\n",
    "import numpy as np\n",
    "import gc\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import CSVLogger\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, accuracy_score, f1_score\n",
    "from ModelArchitecture.DiceLoss import dice_metric_loss\n",
    "from ModelArchitecture import DUCK_Net\n",
    "from ImageLoader import ImageLoader2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1263919",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 952,
     "status": "ok",
     "timestamp": 1702553779333,
     "user": {
      "displayName": "Azimi2kht",
      "userId": "01921100736619125997"
     },
     "user_tz": -210
    },
    "id": "7852dbbf-f3a4-4db6-8c5e-98889bf3abd4",
    "outputId": "4f7fdd6f-1255-4429-c17f-7a3c5f970052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 14:02:23.782989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-05-16 14:02:23.783310: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/azimi2kht/miniconda3/envs/fracatlas-model/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2024-05-16 14:02:23.783406: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/azimi2kht/miniconda3/envs/fracatlas-model/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2024-05-16 14:02:23.783492: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/azimi2kht/miniconda3/envs/fracatlas-model/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2024-05-16 14:02:23.783572: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/azimi2kht/miniconda3/envs/fracatlas-model/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2024-05-16 14:02:23.783655: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/azimi2kht/miniconda3/envs/fracatlas-model/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2024-05-16 14:02:23.783735: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/azimi2kht/miniconda3/envs/fracatlas-model/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2024-05-16 14:02:23.783816: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/azimi2kht/miniconda3/envs/fracatlas-model/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2024-05-16 14:02:23.783897: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/azimi2kht/miniconda3/envs/fracatlas-model/lib/python3.9/site-packages/cv2/../../lib64:\n",
      "2024-05-16 14:02:23.783910: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Checking the number of GPUs available\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07aa714f",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1702553779333,
     "user": {
      "displayName": "Azimi2kht",
      "userId": "01921100736619125997"
     },
     "user_tz": -210
    },
    "id": "9edbe667"
   },
   "outputs": [],
   "source": [
    "# Setting the model parameters\n",
    "\n",
    "img_size = 352\n",
    "dataset_type = 'kvasir' # Options: kvasir/cvc-clinicdb/cvc-colondb/etis-laribpolypdb\n",
    "learning_rate = 1e-4\n",
    "seed_value = 58800\n",
    "filters = 17 # Number of filters, the paper presents the results with 17 and 34\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "ct = datetime.now()\n",
    "\n",
    "model_type = \"DuckNet\"\n",
    "\n",
    "progress_path = 'ProgressFull/' + dataset_type + '_progress_csv_' + model_type + '_filters_' + str(filters) +  '_' + str(ct) + '.csv'\n",
    "progressfull_path = 'ProgressFull/' + dataset_type + '_progress_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.txt'\n",
    "plot_path = 'ProgressFull/' + dataset_type + '_progress_plot_' + model_type + '_filters_' + str(filters) + '_' + str(ct) + '.png'\n",
    "model_path = 'ModelSaveTensorFlow/' + dataset_type + '/' + model_type + '_filters_' + str(filters) + '_' + str(ct)\n",
    "\n",
    "EPOCHS = 600\n",
    "min_loss_for_saving = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a271814",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 502969,
     "status": "ok",
     "timestamp": 1702554282301,
     "user": {
      "displayName": "Azimi2kht",
      "userId": "01921100736619125997"
     },
     "user_tz": -210
    },
    "id": "264050d8",
    "outputId": "1b37bcdd-f2b7-454b-e972-7ee79e0e7d4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing training images and masks: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "X, Y = ImageLoader2D.load_data(img_size, img_size, -1, 'kvasir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1ed4cc9",
   "metadata": {
    "executionInfo": {
     "elapsed": 3589,
     "status": "ok",
     "timestamp": 1702554285866,
     "user": {
      "displayName": "Azimi2kht",
      "userId": "01921100736619125997"
     },
     "user_tz": -210
    },
    "id": "75b6c029"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Splitting the data, seed for reproducibility\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseed_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m x_train, x_valid, y_train, y_valid \u001b[38;5;241m=\u001b[39m train_test_split(x_train, y_train, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.111\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, random_state \u001b[38;5;241m=\u001b[39m seed_value)\n",
      "File \u001b[0;32m~/miniconda3/envs/fracatlas-model/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/fracatlas-model/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2660\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2657\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[1;32m   2659\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2660\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[1;32m   2662\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m   2665\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/fracatlas-model/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2308\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2305\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[1;32m   2307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[1;32m   2312\u001b[0m     )\n\u001b[1;32m   2314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Splitting the data, seed for reproducibility\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, shuffle= True, random_state = seed_value)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.111, shuffle= True, random_state = seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e4dfd",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1702554285867,
     "user": {
      "displayName": "Azimi2kht",
      "userId": "01921100736619125997"
     },
     "user_tz": -210
    },
    "id": "92a7b7d0"
   },
   "outputs": [],
   "source": [
    "# Defining the augmentations\n",
    "\n",
    "aug_train = albu.Compose([\n",
    "    albu.HorizontalFlip(),\n",
    "    albu.VerticalFlip(),\n",
    "    albu.ColorJitter(brightness=(0.6,1.6), contrast=0.2, saturation=0.1, hue=0.01, always_apply=True),\n",
    "    albu.Affine(scale=(0.5,1.5), translate_percent=(-0.125,0.125), rotate=(-180,180), shear=(-22.5,22), always_apply=True),\n",
    "])\n",
    "\n",
    "def augment_images():\n",
    "    x_train_out = []\n",
    "    y_train_out = []\n",
    "\n",
    "    for i in range (len(x_train)):\n",
    "        ug = aug_train(image=x_train[i], mask=y_train[i])\n",
    "        x_train_out.append(ug['image'])\n",
    "        y_train_out.append(ug['mask'])\n",
    "\n",
    "    return np.array(x_train_out), np.array(y_train_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bc0279",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6886,
     "status": "ok",
     "timestamp": 1702554292749,
     "user": {
      "displayName": "Azimi2kht",
      "userId": "01921100736619125997"
     },
     "user_tz": -210
    },
    "id": "1609dd32",
    "outputId": "2a27a292-a3aa-483b-f382-da8afc060e5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DUCK-Net\n"
     ]
    }
   ],
   "source": [
    "# Creating the model\n",
    "\n",
    "model = DUCK_Net.create_model(img_height=img_size, img_width=img_size, input_chanels=3, out_classes=1, starting_filters=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c11e8",
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1702554292749,
     "user": {
      "displayName": "Azimi2kht",
      "userId": "01921100736619125997"
     },
     "user_tz": -210
    },
    "id": "2e513d42"
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=dice_metric_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd031cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ef712b9",
    "outputId": "069e3897-d1e8-4bc9-98aa-6e8cbbd7794d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, epoch 0\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 286s 979ms/step - loss: 0.6326 - val_loss: 0.6759\n",
      "Loss Validation: 0.65623724\n",
      "Loss Test: 0.6208231\n",
      "Training, epoch 1\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 198s 989ms/step - loss: 0.5636 - val_loss: 0.6276\n",
      "Loss Validation: 0.61390686\n",
      "Loss Test: 0.57008326\n",
      "Training, epoch 2\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 986ms/step - loss: 0.5410 - val_loss: 0.6128\n",
      "Loss Validation: 0.61209965\n",
      "Loss Test: 0.5703418\n",
      "Training, epoch 3\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 966ms/step - loss: 0.5305 - val_loss: 0.5661\n",
      "Loss Validation: 0.5612775\n",
      "Loss Test: 0.4947381\n",
      "Training, epoch 4\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 0.5070 - val_loss: 0.5803\n",
      "Loss Validation: 0.57614505\n",
      "Loss Test: 0.49988955\n",
      "Training, epoch 5\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 0.4879 - val_loss: 0.5064\n",
      "Loss Validation: 0.50044745\n",
      "Loss Test: 0.45137477\n",
      "Training, epoch 6\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 967ms/step - loss: 0.4849 - val_loss: 0.5279\n",
      "Loss Validation: 0.5201505\n",
      "Loss Test: 0.4609741\n",
      "Training, epoch 7\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 965ms/step - loss: 0.4848 - val_loss: 0.4808\n",
      "Loss Validation: 0.46765453\n",
      "Loss Test: 0.42584068\n",
      "Training, epoch 8\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 0.4746 - val_loss: 0.4668\n",
      "Loss Validation: 0.4581777\n",
      "Loss Test: 0.43838233\n",
      "Training, epoch 9\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 965ms/step - loss: 0.4555 - val_loss: 0.4934\n",
      "Loss Validation: 0.48362768\n",
      "Loss Test: 0.43843818\n",
      "Training, epoch 10\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 965ms/step - loss: 0.4566 - val_loss: 0.4663\n",
      "Loss Validation: 0.45983702\n",
      "Loss Test: 0.4064126\n",
      "Training, epoch 11\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 984ms/step - loss: 0.4284 - val_loss: 0.4518\n",
      "Loss Validation: 0.45102286\n",
      "Loss Test: 0.42341214\n",
      "Training, epoch 12\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 966ms/step - loss: 0.4237 - val_loss: 0.4583\n",
      "Loss Validation: 0.45024884\n",
      "Loss Test: 0.3934452\n",
      "Training, epoch 13\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 966ms/step - loss: 0.4013 - val_loss: 0.3894\n",
      "Loss Validation: 0.39209312\n",
      "Loss Test: 0.35237646\n",
      "Training, epoch 14\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 984ms/step - loss: 0.3833 - val_loss: 0.3805\n",
      "Loss Validation: 0.375345\n",
      "Loss Test: 0.34467697\n",
      "Training, epoch 15\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 0.3748 - val_loss: 0.3820\n",
      "Loss Validation: 0.3825559\n",
      "Loss Test: 0.29671413\n",
      "Training, epoch 16\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 0.3625 - val_loss: 0.3450\n",
      "Loss Validation: 0.33972865\n",
      "Loss Test: 0.33890152\n",
      "Training, epoch 17\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 965ms/step - loss: 0.3511 - val_loss: 0.3716\n",
      "Loss Validation: 0.3741756\n",
      "Loss Test: 0.3198266\n",
      "Training, epoch 18\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 0.3562 - val_loss: 0.3464\n",
      "Loss Validation: 0.35306627\n",
      "Loss Test: 0.28321362\n",
      "Training, epoch 19\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 966ms/step - loss: 0.3409 - val_loss: 0.3236\n",
      "Loss Validation: 0.334176\n",
      "Loss Test: 0.28350443\n",
      "Training, epoch 20\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 987ms/step - loss: 0.3391 - val_loss: 0.3205\n",
      "Loss Validation: 0.31711555\n",
      "Loss Test: 0.31066245\n",
      "Training, epoch 21\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 0.3180 - val_loss: 0.3173\n",
      "Loss Validation: 0.3164662\n",
      "Loss Test: 0.28668934\n",
      "Training, epoch 22\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 986ms/step - loss: 0.3279 - val_loss: 0.3185\n",
      "Loss Validation: 0.31948197\n",
      "Loss Test: 0.2704237\n",
      "Training, epoch 23\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 985ms/step - loss: 0.3134 - val_loss: 0.2718\n",
      "Loss Validation: 0.27309006\n",
      "Loss Test: 0.25173932\n",
      "Training, epoch 24\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 194s 968ms/step - loss: 0.3204 - val_loss: 0.3453\n",
      "Loss Validation: 0.3394971\n",
      "Loss Test: 0.28230917\n",
      "Training, epoch 25\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 986ms/step - loss: 0.3177 - val_loss: 0.2757\n",
      "Loss Validation: 0.27298564\n",
      "Loss Test: 0.26791155\n",
      "Training, epoch 26\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 967ms/step - loss: 0.2892 - val_loss: 0.2844\n",
      "Loss Validation: 0.2830122\n",
      "Loss Test: 0.24999177\n",
      "Training, epoch 27\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 197s 986ms/step - loss: 0.3019 - val_loss: 0.3235\n",
      "Loss Validation: 0.32616454\n",
      "Loss Test: 0.26009858\n",
      "Training, epoch 28\n",
      "Learning Rate: 0.0001\n",
      "200/200 [==============================] - 193s 966ms/step - loss: 0.2978 - val_loss: 0.2622\n",
      "Loss Validation: 0.2668454\n",
      "Loss Test: 0.24970722\n",
      "Training, epoch 29\n",
      "Learning Rate: 0.0001\n",
      "140/200 [====================>.........] - ETA: 55s - loss: 0.2961"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "step = 0\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "\n",
    "    print(f'Training, epoch {epoch}')\n",
    "    print('Learning Rate: ' + str(learning_rate))\n",
    "\n",
    "    step += 1\n",
    "\n",
    "    image_augmented, mask_augmented = augment_images()\n",
    "\n",
    "    csv_logger = CSVLogger(progress_path, append=True, separator=';')\n",
    "\n",
    "    model.fit(x=image_augmented, y=mask_augmented, epochs=1, batch_size=4, validation_data=(x_valid, y_valid), verbose=1, callbacks=[csv_logger])\n",
    "\n",
    "    prediction_valid = model.predict(x_valid, verbose=0)\n",
    "    loss_valid = dice_metric_loss(y_valid, prediction_valid)\n",
    "\n",
    "    loss_valid = loss_valid.numpy()\n",
    "    print(\"Loss Validation: \" + str(loss_valid))\n",
    "\n",
    "    prediction_test = model.predict(x_test, verbose=0)\n",
    "    loss_test = dice_metric_loss(y_test, prediction_test)\n",
    "    loss_test = loss_test.numpy()\n",
    "    print(\"Loss Test: \" + str(loss_test))\n",
    "\n",
    "    with open(progressfull_path, 'a') as f:\n",
    "        f.write('epoch: ' + str(epoch) + '\\nval_loss: ' + str(loss_valid) + '\\ntest_loss: ' + str(loss_test) + '\\n\\n\\n')\n",
    "\n",
    "    if min_loss_for_saving > loss_valid:\n",
    "        min_loss_for_saving = loss_valid\n",
    "        print(\"Saved model with val_loss: \", loss_valid)\n",
    "        model.save(model_path)\n",
    "\n",
    "    del image_augmented\n",
    "    del mask_augmented\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2d8c8b",
   "metadata": {
    "id": "2cd52447"
   },
   "outputs": [],
   "source": [
    "# Computing the metrics and saving the results\n",
    "\n",
    "print(\"Loading the model\")\n",
    "\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={'dice_metric_loss':dice_metric_loss})\n",
    "\n",
    "prediction_train = model.predict(x_train, batch_size=4)\n",
    "prediction_valid = model.predict(x_valid, batch_size=4)\n",
    "prediction_test = model.predict(x_test, batch_size=4)\n",
    "\n",
    "print(\"Predictions done\")\n",
    "\n",
    "dice_train = f1_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_train > 0.5))\n",
    "dice_test = f1_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                          np.ndarray.flatten(prediction_test > 0.5))\n",
    "dice_valid = f1_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Dice finished\")\n",
    "\n",
    "\n",
    "miou_train = jaccard_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_train > 0.5))\n",
    "miou_test = jaccard_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                          np.ndarray.flatten(prediction_test > 0.5))\n",
    "miou_valid = jaccard_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Miou finished\")\n",
    "\n",
    "\n",
    "precision_train = precision_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                                  np.ndarray.flatten(prediction_train > 0.5))\n",
    "precision_test = precision_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                                 np.ndarray.flatten(prediction_test > 0.5))\n",
    "precision_valid = precision_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                                  np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Precision finished\")\n",
    "\n",
    "\n",
    "recall_train = recall_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                            np.ndarray.flatten(prediction_train > 0.5))\n",
    "recall_test = recall_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                           np.ndarray.flatten(prediction_test > 0.5))\n",
    "recall_valid = recall_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                            np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "print(\"Recall finished\")\n",
    "\n",
    "\n",
    "accuracy_train = accuracy_score(np.ndarray.flatten(np.array(y_train, dtype=bool)),\n",
    "                                np.ndarray.flatten(prediction_train > 0.5))\n",
    "accuracy_test = accuracy_score(np.ndarray.flatten(np.array(y_test, dtype=bool)),\n",
    "                               np.ndarray.flatten(prediction_test > 0.5))\n",
    "accuracy_valid = accuracy_score(np.ndarray.flatten(np.array(y_valid, dtype=bool)),\n",
    "                                np.ndarray.flatten(prediction_valid > 0.5))\n",
    "\n",
    "\n",
    "print(\"Accuracy finished\")\n",
    "\n",
    "\n",
    "final_file = 'results_' + model_type + '_' + str(filters) + '_' + dataset_type + '.txt'\n",
    "print(final_file)\n",
    "\n",
    "with open(final_file, 'a') as f:\n",
    "    f.write(dataset_type + '\\n\\n')\n",
    "    f.write('dice_train: ' + str(dice_train) + ' dice_valid: ' + str(dice_valid) + ' dice_test: ' + str(dice_test) + '\\n\\n')\n",
    "    f.write('miou_train: ' + str(miou_train) + ' miou_valid: ' + str(miou_valid) + ' miou_test: ' + str(miou_test) + '\\n\\n')\n",
    "    f.write('precision_train: ' + str(precision_train) + ' precision_valid: ' + str(precision_valid) + ' precision_test: ' + str(precision_test) + '\\n\\n')\n",
    "    f.write('recall_train: ' + str(recall_train) + ' recall_valid: ' + str(recall_valid) + ' recall_test: ' + str(recall_test) + '\\n\\n')\n",
    "    f.write('accuracy_train: ' + str(accuracy_train) + ' accuracy_valid: ' + str(accuracy_valid) + ' accuracy_test: ' + str(accuracy_test) + '\\n\\n\\n\\n')\n",
    "\n",
    "print('File done')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
